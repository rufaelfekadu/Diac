# Lightning-based configuration for LSTM model
# Extends the base LSTM configuration with Lightning-specific settings

MODEL:
  TYPE: 'Transformer'
  MAXLEN: 272
  VOCAB_SIZE: 77
  ASR_VOCAB_SIZE: 91
  D_MODEL: 128
  NUM_HEADS: 4
  DFF: 128
  NUM_BLOCKS: 2
  DROPOUT_RATE: 0.2
  OUTPUT_SIZE: 19
  USE_ASR: False
  PRETRAINED_PATH: null
  LOAD_TEXT_BRANCH_ONLY: False
  WITH_CONN: False

TRAIN:
  DEVICE: 'cuda'
  BATCH_SIZE: 128
  NUM_EPOCHS: 50
  LEARNING_RATE: 0.001
  SAVE_FREQ: 30
  EVAL_FREQ: 1
  SAVE_DIR: 'checkpoints/transformer-sine-text-only/tashkeela'
  # Lightning-specific settings
  EARLY_STOPPING_PATIENCE: 5
  ACCUMULATE_GRAD_BATCHES: 1
  VAL_CHECK_INTERVAL: 1.0

INFERENCE:
  MAX_LENGTH: 100
  WINDOW_SIZE: 50
  BUFFER_SIZE: 25
  DEVICE: 'cuda'
  BATCH_SIZE: 16
  MODEL_PATH: 'checkpoints/transformer-sine-text-only/tashkeela/best_model.ckpt'
  ASR_MODEL_NAME: 'sashat/whisper-medium-ClassicalAr'
  USE_ASR: False
  FORCED_IDS: null
  OUTPUT_PATH: 'results/transformer-sine-text-only_clartts.txt'

DATA:
  TRAIN_PATH: 'data/tashkeela/train.txt'
  VAL_PATH: 'data/tashkeela/val.txt'
  TEST_PATH: 'data/clartts/test_2.txt'
  MAX_LENGTH: 270

CONSTANTS_PATH: 'constants/'